#!/usr/bin/env python3
import json
import inspect
import re
from typing import Dict, List, Callable, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


class SharedModelManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏"""
    
    _instances = {}
    
    @classmethod
    def get_model(cls, model_name: str):
        """–ü–æ–ª—É—á–∏—Ç—å shared —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏"""
        if model_name not in cls._instances:
            print(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞—é shared –º–æ–¥–µ–ª—å {model_name}...")
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                trust_remote_code=True
            )
            
            # –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if torch.backends.mps.is_available():
                device = "mps"
                print("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Apple Silicon GPU (MPS)")
            else:
                device = "cpu"
                print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "mps" else torch.float32,
                trust_remote_code=True,
                device_map=None
            ).to(device)
            
            model.eval()
            
            cls._instances[model_name] = {
                'tokenizer': tokenizer,
                'model': model,
                'device': device
            }
            print("‚úÖ Shared –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        return cls._instances[model_name]
    
    @classmethod
    def clear_cache(cls):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫–µ—à –º–æ–¥–µ–ª–µ–π"""
        cls._instances.clear()


@dataclass
class ToolFunction:
    """–û–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
    name: str
    description: str
    parameters: Dict[str, Any]
    function: Callable
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.parameters
            }
        }


class NativeToolSystem:
    """–ù–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è Qwen –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_name: str = "Qwen/Qwen3-0.6B"):
        self.model_name = model_name
        self.tools: Dict[str, ToolFunction] = {}
        self.conversation_history: List[Dict[str, Any]] = []
        
        # –ü–æ–ª—É—á–∞–µ–º shared –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print(f"ü§ñ –ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ shared –º–æ–¥–µ–ª–∏ {model_name}...")
        model_components = SharedModelManager.get_model(model_name)
        self.tokenizer = model_components['tokenizer']
        self.model = model_components['model']
        self.device = model_components['device']
        print("‚úÖ NativeToolSystem –≥–æ—Ç–æ–≤")
    
    def register_tool(self, 
                     name: str, 
                     description: str, 
                     parameters: Dict[str, Any], 
                     function: Callable) -> None:
        """
        –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        
        Args:
            name: –ò–º—è —Ñ—É–Ω–∫—Ü–∏–∏
            description: –û–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
            parameters: JSON Schema –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            function: –°–∞–º–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        """
        tool = ToolFunction(name, description, parameters, function)
        self.tools[name] = tool
        print(f"üîß –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {name}")
    
    def tool(self, name: str, description: str, parameters: Dict[str, Any]):
        """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        def decorator(func: Callable):
            self.register_tool(name, description, parameters, func)
            return func
        return decorator
    
    def auto_register_tool(self, func: Callable) -> None:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        name = func.__name__
        description = func.__doc__ or f"–§—É–Ω–∫—Ü–∏—è {name}"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ signature
        sig = inspect.signature(func)
        parameters = {
            "type": "object",
            "properties": {},
            "required": []
        }
        
        for param_name, param in sig.parameters.items():
            param_type = "string"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if param.annotation == int:
                param_type = "integer"
            elif param.annotation == float:
                param_type = "number"
            elif param.annotation == bool:
                param_type = "boolean"
            
            parameters["properties"][param_name] = {
                "type": param_type,
                "description": f"–ü–∞—Ä–∞–º–µ—Ç—Ä {param_name}"
            }
            
            if param.default == inspect.Parameter.empty:
                parameters["required"].append(param_name)
        
        self.register_tool(name, description, parameters, func)
    
    def _format_tools_for_prompt(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        if not self.tools:
            return ""
        
        tools_json = [tool.to_dict() for tool in self.tools.values()]
        return f"\n\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:\n{json.dumps(tools_json, ensure_ascii=False, indent=2)}"
    
    def _create_system_prompt(self) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        system_prompt = """–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º. 

–ö–æ–≥–¥–∞ —Ç–µ–±–µ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞–π –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:
<tool_call>
{"name": "–∏–º—è_—Ñ—É–Ω–∫—Ü–∏–∏", "arguments": {"–ø–∞—Ä–∞–º–µ—Ç—Ä": "–∑–Ω–∞—á–µ–Ω–∏–µ"}}
</tool_call>

–ü–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ç—ã –ø–æ–ª—É—á–∏—à—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ —Å–º–æ–∂–µ—à—å –¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
–ï—Å–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –Ω–µ –Ω—É–∂–Ω—ã, –æ—Ç–≤–µ—á–∞–π –∫–∞–∫ –æ–±—ã—á–Ω–æ."""
        
        tools_info = self._format_tools_for_prompt()
        return system_prompt + tools_info
    
    def _extract_tool_calls(self, text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏"""
        tool_calls = []
        pattern = r'<tool_call>\s*(\{.*?\})\s*</tool_call>'
        
        matches = re.findall(pattern, text, re.DOTALL)
        for match in matches:
            try:
                tool_call = json.loads(match)
                tool_calls.append(tool_call)
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ tool call: {match}")
        
        return tool_calls
    
    def _execute_tool_call(self, tool_call: Dict[str, Any]) -> str:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        tool_name = tool_call.get("name")
        arguments = tool_call.get("arguments", {})
        
        if tool_name not in self.tools:
            return f"‚ùå –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç '{tool_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω"
        
        try:
            tool = self.tools[tool_name]
            result = tool.function(**arguments)
            return f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç {tool_name}: {result}"
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {tool_name}: {str(e)}"
    
    def generate_response(self, 
                         user_input: str, 
                         max_tokens: int = 800,
                         temperature: float = 0.7) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
        messages = [
            {"role": "system", "content": self._create_system_prompt()}
        ] + self.conversation_history
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ chat template
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ tool calls
        tool_calls = self._extract_tool_calls(generated_text)
        
        if tool_calls:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
            tool_results = []
            for tool_call in tool_calls:
                result = self._execute_tool_call(tool_call)
                tool_results.append(result)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.conversation_history.append({
                "role": "assistant",
                "content": generated_text
            })
            
            tool_results_text = "\n".join(tool_results)
            self.conversation_history.append({
                "role": "system",
                "content": f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤:\n{tool_results_text}"
            })
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            messages = [
                {"role": "system", "content": self._create_system_prompt()}
            ] + self.conversation_history + [
                {"role": "user", "content": "–î–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤."}
            ]
            
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            final_response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            self.conversation_history.append({
                "role": "assistant",
                "content": final_response
            })
            
            return final_response
        
        else:
            # –û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            self.conversation_history.append({
                "role": "assistant",
                "content": generated_text
            })
            return generated_text
    
    def clear_history(self):
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        self.conversation_history = []
        print("üßπ –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –æ—á–∏—â–µ–Ω–∞")
    
    def get_registered_tools(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        return list(self.tools.keys())