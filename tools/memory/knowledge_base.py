#!/usr/bin/env python3
"""
–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å FAISS
–•—Ä–∞–Ω–∏—Ç –æ–±—â–∏–µ —Ñ–∞–∫—Ç—ã –∏ –∑–Ω–∞–Ω–∏—è, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
"""

import os
import json
import uuid
import pickle
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import faiss
from core.tool_system import SharedModelManager


@dataclass
class KnowledgeFact:
    """–§–∞–∫—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    id: str
    content: str
    timestamp: str  # YYYY-MM-DD HH:MM:SS
    embedding: Optional[np.ndarray] = None
    source: str = "conversation"
    
    def to_dict(self) -> Dict[str, Any]:
        data = asdict(self)
        # –ù–µ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º embedding –≤ JSON
        if 'embedding' in data:
            del data['embedding']
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'KnowledgeFact':
        return cls(**data)


class KnowledgeBase:
    """–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å FAISS"""
    
    def __init__(self, 
                 model_name: str = "Qwen/Qwen3-0.6B",
                 db_path: str = "knowledge_db",
                 embedding_dim: int = None):
        
        self.model_name = model_name
        self.db_path = db_path
        self.facts: Dict[str, KnowledgeFact] = {}
        
        print(f"üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Knowledge Base —Å shared –º–æ–¥–µ–ª—å—é...")
        
        # –ü–æ–ª—É—á–∞–µ–º shared –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        model_components = SharedModelManager.get_model(model_name)
        self.tokenizer = model_components['tokenizer']
        self.model = model_components['model']
        self.device = model_components['device']
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        if embedding_dim is None:
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            test_embedding = self._create_embedding("test")
            self.embedding_dim = test_embedding.shape[0]
            print(f"üìè –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {self.embedding_dim}")
        else:
            self.embedding_dim = embedding_dim
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º FAISS –∏–Ω–¥–µ–∫—Å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é
        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–î
        self._load_database()
        
        print("‚úÖ Knowledge Base –≥–æ—Ç–æ–≤–∞")
    
    def _create_embedding(self, text: str) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            # –î–ª—è causal LM –∏—Å–ø–æ–ª—å–∑—É–µ–º hidden states –∏–ª–∏ logits
            if hasattr(outputs, 'hidden_states') and outputs.hidden_states:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state
                embeddings = outputs.hidden_states[-1].mean(dim=1)
            elif hasattr(outputs, 'last_hidden_state'):
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è encoder –º–æ–¥–µ–ª–µ–π
                embeddings = outputs.last_hidden_state.mean(dim=1)
            else:
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º logits –∫–∞–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                embeddings = outputs.logits.mean(dim=1)
                
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        
        return embeddings.cpu().numpy().astype('float32')[0]
    
    def _save_database(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        os.makedirs(self.db_path, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–∫—Ç—ã
        facts_data = {fact_id: fact.to_dict() for fact_id, fact in self.facts.items()}
        with open(os.path.join(self.db_path, "facts.json"), "w", encoding="utf-8") as f:
            json.dump(facts_data, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        if self.index.ntotal > 0:
            faiss.write_index(self.index, os.path.join(self.db_path, "faiss.index"))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ ID –¥–ª—è FAISS
        fact_ids = list(self.facts.keys())
        with open(os.path.join(self.db_path, "fact_ids.pkl"), "wb") as f:
            pickle.dump(fact_ids, f)
        
        print(f"üíæ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {len(self.facts)} —Ñ–∞–∫—Ç–æ–≤")
    
    def _load_database(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        if not os.path.exists(self.db_path):
            print("üìù –°–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
            return
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–∫—Ç—ã
            facts_path = os.path.join(self.db_path, "facts.json")
            if os.path.exists(facts_path):
                with open(facts_path, "r", encoding="utf-8") as f:
                    facts_data = json.load(f)
                
                self.facts = {
                    fact_id: KnowledgeFact.from_dict(fact_data)
                    for fact_id, fact_data in facts_data.items()
                }
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            index_path = os.path.join(self.db_path, "faiss.index")
            if os.path.exists(index_path):
                self.index = faiss.read_index(index_path)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ ID
            ids_path = os.path.join(self.db_path, "fact_ids.pkl")
            if os.path.exists(ids_path):
                with open(ids_path, "rb") as f:
                    self.fact_ids = pickle.load(f)
            else:
                self.fact_ids = list(self.facts.keys())
            
            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {len(self.facts)} —Ñ–∞–∫—Ç–æ–≤")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ë–î: {e}. –°–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è.")
            self.facts = {}
            self.index = faiss.IndexFlatIP(self.embedding_dim)
            self.fact_ids = []
    
    def add_fact(self, content: str) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        existing_facts = self.search_facts(content, limit=1, threshold=0.95)
        if existing_facts and existing_facts[0][1] > 0.95:
            return f"‚ùå –ü–æ—Ö–æ–∂–∏–π —Ñ–∞–∫—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {existing_facts[0][0].content[:50]}..."
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–∫—Ç
        fact = KnowledgeFact(
            id=str(uuid.uuid4()),
            content=content,
            timestamp=datetime.now().isoformat()
        )
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        fact.embedding = self._create_embedding(content)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        self.facts[fact.id] = fact
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ FAISS –∏–Ω–¥–µ–∫—Å
        if fact.embedding is not None:
            self.index.add(fact.embedding.reshape(1, -1))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ ID
        if not hasattr(self, 'fact_ids'):
            self.fact_ids = []
        self.fact_ids.append(fact.id)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ë–î
        #self._save_database()
        
        return f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Ñ–∞–∫—Ç: {content[:50]}... (ID: {fact.id[:8]})"
    
    def search_facts(self, 
                    query: str, 
                    limit: int = 5, 
                    threshold: float = 0.7) -> List[Tuple[KnowledgeFact, float]]:
        """–ü–æ–∏—Å–∫ —Ñ–∞–∫—Ç–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É"""
        
        if self.index.ntotal == 0:
            return []
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self._create_embedding(query)
        
        # –ü–æ–∏—Å–∫ –≤ FAISS
        scores, indices = self.index.search(query_embedding.reshape(1, -1), min(limit * 2, self.index.ntotal))
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx >= len(self.fact_ids):
                continue
                
            fact_id = self.fact_ids[idx]
            if fact_id not in self.facts:
                continue
                
            fact = self.facts[fact_id]
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É
            if score >= threshold:
                results.append((fact, float(score)))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:limit]
    
    def delete_fact(self, fact_id: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞"""
        if fact_id not in self.facts:
            return f"‚ùå –§–∞–∫—Ç —Å ID {fact_id} –Ω–µ –Ω–∞–π–¥–µ–Ω"
        
        fact = self.facts[fact_id]
        del self.facts[fact_id]
        
        # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ FAISS –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
        self._rebuild_index()
        
        return f"‚úÖ –£–¥–∞–ª–µ–Ω —Ñ–∞–∫—Ç: {fact.content[:50]}..."
    
    def _rebuild_index(self):
        """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        self.index = faiss.IndexFlatIP(self.embedding_dim)
        self.fact_ids = []
        
        for fact_id, fact in self.facts.items():
            if fact.embedding is not None:
                self.index.add(fact.embedding.reshape(1, -1))
                self.fact_ids.append(fact_id)
        
        #self._save_database()
    
    def get_knowledge_context(self, query: str, max_facts: int = 3) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        relevant_facts = self.search_facts(query, limit=max_facts, threshold=0.6)
        
        if not relevant_facts:
            return ""
        
        context_parts = ["=== –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ó–ù–ê–ù–ò–Ø ==="]
        for fact, score in relevant_facts:
            context_parts.append(
                f"{fact.content} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f})"
            )
        context_parts.append("=== –ö–û–ù–ï–¶ –ó–ù–ê–ù–ò–ô ===")
        
        return "\n".join(context_parts)
    
    def auto_add_facts(self, text: str) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        facts = self.extract_facts_from_text(text)
        added_count = 0
        
        for fact in facts:
            try:
                result = self.add_fact(fact)
                if "‚úÖ" in result:
                    added_count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ñ–∞–∫—Ç–∞: {e}")
        
        if added_count > 0:
            return f"üß† –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ {added_count} —Ñ–∞–∫—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
        else:
            return ""
    
    def extract_facts_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å NER –∏–ª–∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        
        potential_facts = []
        sentences = text.split('.')
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                continue
            
            # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –¥–∞—Ç–∞–º–∏, –º–µ—Å—Ç–∞–º–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏
            indicators = [
                '—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–ø—Ä–æ–∏–∑–æ—à–ª–æ',
                '–±—ã–ª —Å–æ–∑–¥–∞–Ω', '–∏–∑–æ–±—Ä–µ—Ç–µ–Ω', '–æ—Å–Ω–æ–≤–∞–Ω', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω'
            ]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–∏—Å–∫–ª—é—á–∞–µ–º)
            personal_indicators = [
                '—è', '–º–Ω–µ', '–º–æ–π', '–º–æ—è', '–º–æ–µ', '–º–æ–∏', '–º–µ–Ω—è', '–º–Ω–æ–π'
            ]
            
            sentence_lower = sentence.lower()
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if any(indicator in sentence_lower for indicator in personal_indicators):
                continue
            
            # –ò—â–µ–º —Ñ–∞–∫—Ç—ã
            if (any(indicator in sentence_lower for indicator in indicators) or
                any(char.isdigit() for char in sentence) or  # –°–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–∞/–¥–∞—Ç—ã
                len(sentence) > 30):  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                
                potential_facts.append(sentence)
        
        return potential_facts[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 —Ñ–∞–∫—Ç–∞ –∑–∞ —Ä–∞–∑
    
    def get_stats(self) -> str:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        if not self.facts:
            return "üìä –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞"
        
        stats = [
            f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:",
            f"  üìö –í—Å–µ–≥–æ —Ñ–∞–∫—Ç–æ–≤: {len(self.facts)}",
            f"  üíæ FAISS –∏–Ω–¥–µ–∫—Å: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤",
            f"  üïí –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {max(fact.timestamp for fact in self.facts.values())[:19]}"
        ]
        
        return "\n".join(stats)


def create_knowledge_tools(knowledge_base: KnowledgeBase) -> Dict[str, Dict[str, Any]]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π"""
    
    tools = {}
    
    # 1. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞
    tools["knowledge_add_fact"] = {
        "name": "knowledge_add_fact",
        "description": "–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—â–∏–π —Ñ–∞–∫—Ç –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π (–Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)",
        "parameters": {
            "type": "object",
            "properties": {
                "content": {
                    "type": "string",
                    "description": "–§–∞–∫—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
                }
            },
            "required": ["content"]
        },
        "function": knowledge_base.add_fact
    }
    
    # 2. –ü–æ–∏—Å–∫ —Ñ–∞–∫—Ç–æ–≤
    tools["knowledge_search"] = {
        "name": "knowledge_search",
        "description": "–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"
                },
                "limit": {
                    "type": "integer",
                    "minimum": 1,
                    "maximum": 10,
                    "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
                }
            },
            "required": ["query"]
        },
        "function": lambda query, limit=5: _search_facts_formatted(
            knowledge_base, query, limit
        )
    }
    
    # 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    tools["knowledge_stats"] = {
        "name": "knowledge_stats",
        "description": "–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π",
        "parameters": {
            "type": "object",
            "properties": {},
            "required": []
        },
        "function": knowledge_base.get_stats
    }
    
    return tools


def _search_facts_formatted(knowledge_base: KnowledgeBase, 
                          query: str, 
                          limit: int = 5) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ñ–∞–∫—Ç–æ–≤"""
    
    results = knowledge_base.search_facts(query, limit)
    
    if not results:
        return f"‚ùå –§–∞–∫—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    
    formatted_results = [f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ñ–∞–∫—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':"]
    
    for i, (fact, score) in enumerate(results, 1):
        formatted_results.append(
            f"{i}. {fact.content} "
            f"(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}, –¥–æ–±–∞–≤–ª–µ–Ω: {fact.timestamp[:10]})"
        )
    
    return "\n".join(formatted_results)